{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2d16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded scaler: scaler_pignn_h24.npz\n",
      "Device: cpu | Cities: 29 | k=4 | Avg deg=4.90\n",
      "Loaded DET warm-start from: ckpt_pignn_h24.pt\n",
      "Note: logsigma_head is NEW and will be trained.\n",
      "[UQ h=24] ep 01 | train 96.7584 | val_loss 7.3220 | val_viol 33.44% | D 0.5019 k 0.3113 | lr 0.002\n",
      "[UQ h=24] ep 02 | train 4.6529 | val_loss 7.0159 | val_viol 33.44% | D 0.5019 k 0.3113 | lr 0.002\n",
      "[UQ h=24] ep 03 | train 4.4459 | val_loss 6.9707 | val_viol 33.44% | D 0.5019 k 0.3113 | lr 0.002\n",
      "[UQ h=24] ep 04 | train 3.5661 | val_loss 4.2010 | val_viol 13.31% | D 0.5247 k 0.3087 | lr 0.0005\n",
      "[UQ h=24] ep 05 | train 3.4876 | val_loss 4.0969 | val_viol 11.23% | D 0.5459 k 0.3052 | lr 0.0005\n",
      "[UQ h=24] ep 06 | train 3.4617 | val_loss 4.1992 | val_viol 12.89% | D 0.5621 k 0.3025 | lr 0.0005\n",
      "[UQ h=24] ep 07 | train 3.4423 | val_loss 4.4552 | val_viol 17.21% | D 0.5780 k 0.3000 | lr 0.0005\n",
      "[UQ h=24] ep 08 | train 3.4299 | val_loss 4.2353 | val_viol 13.87% | D 0.5894 k 0.2984 | lr 0.0005\n",
      "[UQ h=24] ep 09 | train 3.4162 | val_loss 4.3649 | val_viol 14.73% | D 0.6019 k 0.2963 | lr 0.0005\n",
      "[UQ h=24] ep 10 | train 3.4085 | val_loss 4.3397 | val_viol 15.19% | D 0.6090 k 0.2948 | lr 0.0005\n",
      "Early stop. Best val_loss=4.0969\n",
      "Saved UQ checkpoint: ckpt_pignn_uq_h24.pt\n",
      "\n",
      "MC inference on VAL...\n",
      "VAL rows: 125976\n",
      "\n",
      "MC inference on TEST...\n",
      "TEST rows: 100920\n",
      "\n",
      "Saved: predictions_uq_h24.csv\n",
      "tau_star (VAL max-F1) = 0.40 (F1=0.778) | tau_policy=0.80\n",
      "Saved: metrics_uq_h24.csv\n",
      "          p_col  tau     brier           ece     auprc  precision    recall  \\\n",
      "0  p_exceed_cal  0.4  0.046648  1.098985e-02  0.658157   0.675851  0.578839   \n",
      "1  p_exceed_cal  0.8  0.046648  1.098985e-02  0.658157   0.845262  0.134841   \n",
      "2  p_exceed_raw  0.4  0.062141  5.713449e-02  0.664474   0.868878  0.059809   \n",
      "3  p_exceed_raw  0.8  0.062141  5.713449e-02  0.664474   0.000000  0.000000   \n",
      "4  p_exceed_cal  0.4  0.092214  3.349221e-17  0.853279   0.738938  0.821239   \n",
      "5  p_exceed_cal  0.8  0.092214  3.349221e-17  0.853279   0.918040  0.455059   \n",
      "6  p_exceed_raw  0.4  0.148063  1.578735e-01  0.856734   0.940255  0.346803   \n",
      "7  p_exceed_raw  0.8  0.148063  1.578735e-01  0.856734   0.000000  0.000000   \n",
      "\n",
      "         f1     tp     fp     tn     fn split  T_exceed  tau_policy  \\\n",
      "0  0.623594   5323   2553  89171   3873  test      65.0         0.8   \n",
      "1  0.232580   1240    227  91497   7956  test      65.0         0.8   \n",
      "2  0.111914    550     83  91641   8646  test      65.0         0.8   \n",
      "3  0.000000      0      0  91724   9196  test      65.0         0.8   \n",
      "4  0.777918  30261  10691  78437   6587   val      65.0         0.8   \n",
      "5  0.608495  16768   1497  87631  20080   val      65.0         0.8   \n",
      "6  0.506711  12779    812  88316  24069   val      65.0         0.8   \n",
      "7  0.000000      0      0  89128  36848   val      65.0         0.8   \n",
      "\n",
      "   tau_star_val  \n",
      "0           0.4  \n",
      "1           0.4  \n",
      "2           0.4  \n",
      "3           0.4  \n",
      "4           0.4  \n",
      "5           0.4  \n",
      "6           0.4  \n",
      "7           0.4  \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#   1) Rebuilds the SAME features/graph/time-tensor as deterministic GS-ADR h=24 run\n",
    "#   2) Loads ckpt_pignn_h24.pt + scaler_pignn_h24.npz\n",
    "#   3) Adds a heteroscedastic aleatoric head (sigma) and trains with Gaussian NLL\n",
    "#      - Freeze mean/physics/embeddings for a few epochs (sigma-only warmup)\n",
    "#      - Then fine-tune all parameters lightly\n",
    "#   4) MC-dropout inference (M=30) = epistemic + aleatoric + exceedance probability\n",
    "#   5) Isotonic calibration on VAL for p_exceed\n",
    "\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import (\n",
    "    brier_score_loss, precision_recall_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DATA_PATH = \"dataset_2023_2025.csv\"\n",
    "\n",
    "CKPT_DET_PATH   = \"ckpt_pignn_h24.pt\"\n",
    "SCALER_PATH     = \"scaler_pignn_h24.npz\"         \n",
    "OUT_PRED_UQ     = \"predictions_uq_h24.csv\"\n",
    "OUT_MET_UQ      = \"metrics_uq_h24.csv\"\n",
    "CKPT_UQ_PATH    = \"ckpt_pignn_uq_h24.pt\"\n",
    "\n",
    "# Graph\n",
    "K_GRAPH = 4\n",
    "ELL_KM  = 120.0\n",
    "\n",
    "# Horizon\n",
    "H = 24\n",
    "DT = 1.0\n",
    "\n",
    "# Feature design (must match the deterministic model\n",
    "PM_LAGS   = [1, 2, 6, 24]\n",
    "CHEMS     = ['carbon_monoxide', 'nitrogen_dioxide', 'sulphur_dioxide', 'ozone']\n",
    "CHEM_LAGS = [1, 6, 24]\n",
    "\n",
    "# Splits (same strict time split)\n",
    "TRAIN_END = \"2024-12-31 23:00:00\"\n",
    "VAL_END   = \"2025-06-30 23:00:00\"\n",
    "TEST_END  = \"2025-11-23 23:00:00\"\n",
    "\n",
    "# Constraints (apply on mean mu)\n",
    "LAMBDA_INEQ    = 1.0\n",
    "LAMBDA_NONNEG  = 0.05\n",
    "\n",
    "# Model capacity (must match deterministic)\n",
    "DROPOUT = 0.20\n",
    "EMB_DIM = 8\n",
    "HIDDEN  = 96\n",
    "\n",
    "# Training\n",
    "SEED = 42\n",
    "BATCH_TIMES = 128\n",
    "\n",
    "EPOCHS_TOTAL = 25\n",
    "FREEZE_MEAN_EPOCHS = 3     # sigma-only warmup\n",
    "PATIENCE = 5\n",
    "\n",
    "LR_SIGMA  = 2e-3           # during freeze\n",
    "LR_FINE   = 5e-4           # after unfreeze (smaller)\n",
    "WEIGHT_DECAY = 1e-6\n",
    "\n",
    "# UQ inference\n",
    "M_MC = 30\n",
    "\n",
    "# Policy threshold\n",
    "T_EXCEED   = 65.0\n",
    "TAU_POLICY = 0.80          # policy (strict); we will also compute tau_star on VAL\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def time_split_times(times):\n",
    "    times = pd.to_datetime(times)\n",
    "    tr = times[times <= pd.Timestamp(TRAIN_END)]\n",
    "    va = times[(times > pd.Timestamp(TRAIN_END)) & (times <= pd.Timestamp(VAL_END))]\n",
    "    te = times[(times > pd.Timestamp(VAL_END)) & (times <= pd.Timestamp(TEST_END))]\n",
    "    return tr, va, te\n",
    "\n",
    "def build_knn_graph(city_meta, k=4, ell_km=120.0):\n",
    "    coords = city_meta[['lat','lon']].to_numpy()\n",
    "    coords_rad = np.radians(coords)\n",
    "\n",
    "    nnm = NearestNeighbors(n_neighbors=k+1, metric='haversine')\n",
    "    nnm.fit(coords_rad)\n",
    "    dists, idxs = nnm.kneighbors(coords_rad)\n",
    "\n",
    "    n = len(city_meta)\n",
    "    W = np.zeros((n,n), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        for t in range(1, k+1):\n",
    "            j = idxs[i, t]\n",
    "            dist_km = dists[i, t] * 6371.0\n",
    "            w = np.exp(-(dist_km/ell_km)**2)\n",
    "            W[i, j] = max(W[i, j], w)\n",
    "            W[j, i] = max(W[j, i], w)\n",
    "    D = np.diag(W.sum(axis=1))\n",
    "    L = (D - W).astype(np.float32)\n",
    "    return W, L\n",
    "\n",
    "def hinge_pos(x): return F.relu(x)\n",
    "\n",
    "def enable_dropout(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()\n",
    "\n",
    "def normal_cdf(z):\n",
    "    return 0.5 * (1.0 + torch.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "def gaussian_nll(mu, sigma, y):\n",
    "    # sigma positive; returns (B,N,1)\n",
    "    var = sigma**2\n",
    "    return 0.5 * (torch.log(var + 1e-12) + (y - mu)**2 / (var + 1e-12))\n",
    "\n",
    "def ece_score(p, y, n_bins=10):\n",
    "    p = np.asarray(p); y = np.asarray(y)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        mask = (p >= lo) & (p < hi) if i < n_bins-1 else (p >= lo) & (p <= hi)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        conf = p[mask].mean()\n",
    "        acc  = y[mask].mean()\n",
    "        ece += (mask.sum() / len(p)) * abs(acc - conf)\n",
    "    return float(ece)\n",
    "\n",
    "def pick_tau_by_val(df_val, p_col):\n",
    "    y = df_val[\"z_true\"].values.astype(int)\n",
    "    p = df_val[p_col].values.astype(float)\n",
    "    taus = np.linspace(0.05, 0.95, 19)\n",
    "    best = (-1, 0.5)\n",
    "    for tau in taus:\n",
    "        yhat = (p >= tau).astype(int)\n",
    "        f1 = f1_score(y, yhat, zero_division=0)\n",
    "        if f1 > best[0]:\n",
    "            best = (f1, float(tau))\n",
    "    return float(best[1]), float(best[0])\n",
    "\n",
    "def compute_metrics(df_split, p_col, tau):\n",
    "    y = df_split[\"z_true\"].values.astype(int)\n",
    "    p = df_split[p_col].values.astype(float)\n",
    "\n",
    "    bs = brier_score_loss(y, p)\n",
    "    ece = ece_score(p, y, n_bins=10)\n",
    "\n",
    "    auprc = average_precision_score(y, p)\n",
    "\n",
    "    yhat = (p >= tau).astype(int)\n",
    "    P = precision_score(y, yhat, zero_division=0)\n",
    "    R = recall_score(y, yhat, zero_division=0)\n",
    "    F1 = f1_score(y, yhat, zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, yhat).ravel()\n",
    "\n",
    "    return {\n",
    "        \"p_col\": p_col,\n",
    "        \"tau\": float(tau),\n",
    "        \"brier\": float(bs),\n",
    "        \"ece\": float(ece),\n",
    "        \"auprc\": float(auprc),\n",
    "        \"precision\": float(P),\n",
    "        \"recall\": float(R),\n",
    "        \"f1\": float(F1),\n",
    "        \"tp\": int(tp), \"fp\": int(fp), \"tn\": int(tn), \"fn\": int(fn),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Load deterministic scaler\n",
    "# -----------------------------\n",
    "class SavedStandardScaler:\n",
    "    def __init__(self, mean_, scale_):\n",
    "        self.mean_ = mean_.astype(np.float32)\n",
    "        self.scale_ = scale_.astype(np.float32)\n",
    "        self.scale_[self.scale_ == 0] = 1.0\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X (..., F)\n",
    "        return (X - self.mean_) / self.scale_\n",
    "\n",
    "sc_npz = np.load(SCALER_PATH)\n",
    "scaler = SavedStandardScaler(sc_npz[\"mean_\"], sc_npz[\"scale_\"])\n",
    "print(\"Loaded scaler:\", SCALER_PATH)\n",
    "\n",
    "# Load & build features (must match DET)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "keep_cols = [\n",
    "    \"city_id\",\"city_name\",\"lat\",\"lon\",\"datetime\",\n",
    "    \"pm2_5\",\"pm10\",\n",
    "] + CHEMS\n",
    "df = df[keep_cols].copy().sort_values([\"city_id\",\"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "# lags (history only)\n",
    "for lag in PM_LAGS:\n",
    "    df[f\"pm2_5_lag{lag}\"] = df.groupby(\"city_id\")[\"pm2_5\"].shift(lag)\n",
    "for c in CHEMS:\n",
    "    for lag in CHEM_LAGS:\n",
    "        df[f\"{c}_lag{lag}\"] = df.groupby(\"city_id\")[c].shift(lag)\n",
    "\n",
    "# targets at h=24\n",
    "df[f\"y_h{H}\"] = df.groupby(\"city_id\")[\"pm2_5\"].shift(-H)\n",
    "df[f\"pm10_h{H}\"] = df.groupby(\"city_id\")[\"pm10\"].shift(-H)\n",
    "\n",
    "needed = [f\"pm2_5_lag{l}\" for l in PM_LAGS] + \\\n",
    "         [f\"{c}_lag{l}\" for c in CHEMS for l in CHEM_LAGS] + \\\n",
    "         [f\"y_h{H}\", f\"pm10_h{H}\"]\n",
    "df = df.dropna(subset=needed).copy()\n",
    "\n",
    "# city order\n",
    "city_meta = df[[\"city_id\",\"city_name\",\"lat\",\"lon\"]].drop_duplicates().sort_values(\"city_id\").reset_index(drop=True)\n",
    "city_ids = city_meta[\"city_id\"].to_numpy()\n",
    "cid_to_idx = {cid:i for i,cid in enumerate(city_ids)}\n",
    "N = len(city_ids)\n",
    "\n",
    "# graph\n",
    "W_np, L_np = build_knn_graph(city_meta, k=K_GRAPH, ell_km=ELL_KM)\n",
    "W = torch.tensor(W_np, device=DEVICE)\n",
    "L = torch.tensor(L_np, device=DEVICE)\n",
    "deg = W.sum(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "A_norm = W / deg\n",
    "\n",
    "print(f\"Device: {DEVICE} | Cities: {N} | k={K_GRAPH} | Avg deg={(W_np>0).sum(axis=1).mean():.2f}\")\n",
    "\n",
    "# time tensor (target-time cyclical at t+h, no leakage)\n",
    "def build_time_tensor(df_in, horizon_h):\n",
    "    df2 = df_in.copy()\n",
    "    df2[\"cid_idx\"] = df2[\"city_id\"].map(cid_to_idx)\n",
    "    df2 = df2.sort_values([\"datetime\",\"cid_idx\"])\n",
    "\n",
    "    # target-time cyc features at t+h\n",
    "    dt_tgt = df2[\"datetime\"] + pd.to_timedelta(horizon_h, unit=\"h\")\n",
    "    hour = dt_tgt.dt.hour.values\n",
    "    doy  = dt_tgt.dt.dayofyear.values\n",
    "\n",
    "    df2[\"hour_sin_tgt\"] = np.sin(2*np.pi*hour/24.0)\n",
    "    df2[\"hour_cos_tgt\"] = np.cos(2*np.pi*hour/24.0)\n",
    "    df2[\"doy_sin_tgt\"]  = np.sin(2*np.pi*doy/365.0)\n",
    "    df2[\"doy_cos_tgt\"]  = np.cos(2*np.pi*doy/365.0)\n",
    "\n",
    "    X_COLS = []\n",
    "    X_COLS += CHEMS\n",
    "    X_COLS += [f\"{c}_lag{l}\" for c in CHEMS for l in CHEM_LAGS]\n",
    "    X_COLS += [f\"pm2_5_lag{l}\" for l in PM_LAGS]\n",
    "    X_COLS += [\"doy_sin_tgt\",\"doy_cos_tgt\",\"hour_sin_tgt\",\"hour_cos_tgt\"]\n",
    "    X_COLS += [\"lat\",\"lon\"]\n",
    "\n",
    "    # keep only full times (all cities)\n",
    "    counts = df2.groupby(\"datetime\")[\"cid_idx\"].nunique()\n",
    "    full_times = counts[counts == N].index\n",
    "    df2 = df2[df2[\"datetime\"].isin(full_times)].copy()\n",
    "\n",
    "    times = pd.to_datetime(np.sort(df2[\"datetime\"].unique()))\n",
    "    T = len(times)\n",
    "\n",
    "    X = np.zeros((T, N, len(X_COLS)), dtype=np.float32)\n",
    "    c0 = np.zeros((T, N, 1), dtype=np.float32)\n",
    "    y  = np.zeros((T, N, 1), dtype=np.float32)\n",
    "    pm10_tgt = np.zeros((T, N, 1), dtype=np.float32)\n",
    "\n",
    "    g = df2.groupby(\"datetime\", sort=True)\n",
    "    for ti, t in enumerate(times):\n",
    "        gt = g.get_group(t).sort_values(\"cid_idx\")\n",
    "        X[ti,:,:] = gt[X_COLS].to_numpy(np.float32)\n",
    "        c0[ti,:,0] = gt[\"pm2_5\"].to_numpy(np.float32)\n",
    "        y[ti,:,0]  = gt[f\"y_h{horizon_h}\"].to_numpy(np.float32)\n",
    "        pm10_tgt[ti,:,0] = gt[f\"pm10_h{horizon_h}\"].to_numpy(np.float32)\n",
    "\n",
    "    return times, X, c0, y, pm10_tgt, X_COLS\n",
    "\n",
    "times, X, c0, y, pm10_tgt, X_COLS = build_time_tensor(df, H)\n",
    "\n",
    "tr_times, va_times, te_times = time_split_times(times)\n",
    "time_to_i = {t:i for i,t in enumerate(times)}\n",
    "tr_idx = np.array([time_to_i[t] for t in tr_times], dtype=int)\n",
    "va_idx = np.array([time_to_i[t] for t in va_times], dtype=int)\n",
    "te_idx = np.array([time_to_i[t] for t in te_times], dtype=int)\n",
    "\n",
    "# apply saved scaler (train-only) from deterministic stage\n",
    "X = scaler.transform(X).astype(np.float32)\n",
    "\n",
    "# Model (must match DET mean pathway) + sigma head\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=96, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class PIGNN_UQ(nn.Module):\n",
    "    \"\"\"\n",
    "    Mean: same PI-GNN implicit macro-step with learned source mean s_mean\n",
    "    Sigma: heteroscedastic head from same backbone (aleatoric)\n",
    "    Epistemic: MC-dropout at inference\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, L, A_norm, n_cities, emb_dim=8, hidden=96, dt=1.0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.A = A_norm\n",
    "        self.dt = dt\n",
    "\n",
    "        self.city_emb = nn.Embedding(n_cities, emb_dim)\n",
    "        self.register_buffer(\"city_idx\", torch.arange(n_cities, dtype=torch.long))\n",
    "\n",
    "        # input to backbone: [x, x_nb, c0, c_nb, emb]\n",
    "        self.in_dim = 2*x_dim + 2*1 + emb_dim\n",
    "        self.backbone = Backbone(self.in_dim, hidden=hidden, dropout=dropout)\n",
    "\n",
    "        # mean head (source term mean)\n",
    "        self.mean_head = nn.Linear(hidden, 1)\n",
    "\n",
    "        # aleatoric head\n",
    "        self.logsig_head = nn.Linear(hidden, 1)\n",
    "        # init sigma small (important for stability)\n",
    "        nn.init.zeros_(self.logsig_head.weight)\n",
    "        nn.init.constant_(self.logsig_head.bias, -2.0)  # softplus(-2) ~ 0.13\n",
    "\n",
    "        # physics params\n",
    "        self.D_raw = nn.Parameter(torch.tensor(0.0))\n",
    "        self.k_raw = nn.Parameter(torch.tensor(-1.0))\n",
    "\n",
    "    def D(self): return F.softplus(self.D_raw) + 1e-6\n",
    "    def k(self): return F.softplus(self.k_raw) + 1e-6\n",
    "\n",
    "    def forward(self, c0, x, steps):\n",
    "        # neighbor agg\n",
    "        x_nb = torch.einsum(\"ij,bjk->bik\", self.A, x)\n",
    "        c_nb = torch.einsum(\"ij,bjk->bik\", self.A, c0)\n",
    "\n",
    "        emb = self.city_emb(self.city_idx).unsqueeze(0).expand(x.shape[0], -1, -1)\n",
    "\n",
    "        z = torch.cat([x, x_nb, c0, c_nb, emb], dim=-1)\n",
    "        h = self.backbone(z)\n",
    "\n",
    "        s_mean = self.mean_head(h)\n",
    "        sigma  = F.softplus(self.logsig_head(h)) + 1e-3   # positive\n",
    "\n",
    "        # implicit macro-step (same as DET)\n",
    "        D = self.D()\n",
    "        k = self.k()\n",
    "        hdt = steps * self.dt\n",
    "\n",
    "        Nn = self.L.shape[0]\n",
    "        I = torch.eye(Nn, device=c0.device, dtype=c0.dtype)\n",
    "        A_sys = I + hdt * (D * self.L + k * I)\n",
    "        A_b = A_sys.unsqueeze(0).expand(c0.shape[0], -1, -1)\n",
    "        rhs = c0 + hdt * s_mean\n",
    "\n",
    "        mu = torch.linalg.solve(A_b, rhs)  # (B,N,1)\n",
    "        return mu, sigma\n",
    "\n",
    "# -----------------------------\n",
    "# Load deterministic checkpoint into UQ model (warm-start mean & physics)\n",
    "# -----------------------------\n",
    "model = PIGNN_UQ(\n",
    "    x_dim=X.shape[-1], L=L, A_norm=A_norm, n_cities=N,\n",
    "    emb_dim=EMB_DIM, hidden=HIDDEN, dt=DT, dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "ckpt = torch.load(CKPT_DET_PATH, map_location=DEVICE)\n",
    "\n",
    "# The deterministic stage likely saved either:\n",
    "#  - a full model.state_dict(), or\n",
    "#  - a dict with key \"state_dict\".\n",
    "state = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "# Map compatible keys:\n",
    "missing, unexpected = [], []\n",
    "\n",
    "def load_if_exists(dst_key, src_key):\n",
    "    if src_key in state and dst_key in model.state_dict():\n",
    "        model.state_dict()[dst_key].copy_(state[src_key])\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "sd = model.state_dict()\n",
    "\n",
    "# embeddings\n",
    "load_if_exists(\"city_emb.weight\", \"city_emb.weight\")\n",
    "\n",
    "# physics params\n",
    "load_if_exists(\"D_raw\", \"D_raw\")\n",
    "load_if_exists(\"k_raw\", \"k_raw\")\n",
    "\n",
    "# If deterministic had a single SourceNet with .backbone and .mean_head we can match:\n",
    "# try several likely key patterns\n",
    "key_map_attempts = [\n",
    "    # DET (newer) style\n",
    "    (\"backbone.net.0.weight\", \"source.backbone.0.weight\"),\n",
    "    (\"backbone.net.0.bias\",   \"source.backbone.0.bias\"),\n",
    "    (\"backbone.net.3.weight\", \"source.backbone.3.weight\"),\n",
    "    (\"backbone.net.3.bias\",   \"source.backbone.3.bias\"),\n",
    "    (\"mean_head.weight\",      \"source.mean_head.weight\"),\n",
    "    (\"mean_head.bias\",        \"source.mean_head.bias\"),\n",
    "\n",
    "    # DET (older) style: source is nn.Sequential and last is Linear->1\n",
    "    (\"backbone.net.0.weight\", \"source.net.0.weight\"),\n",
    "    (\"backbone.net.0.bias\",   \"source.net.0.bias\"),\n",
    "    (\"backbone.net.3.weight\", \"source.net.3.weight\"),\n",
    "    (\"backbone.net.3.bias\",   \"source.net.3.bias\"),\n",
    "    (\"mean_head.weight\",      \"source.net.6.weight\"),\n",
    "    (\"mean_head.bias\",        \"source.net.6.bias\"),\n",
    "]\n",
    "\n",
    "loaded_any = False\n",
    "for dst_k, src_k in key_map_attempts:\n",
    "    if dst_k in sd and src_k in state:\n",
    "        sd[dst_k].copy_(state[src_k])\n",
    "        loaded_any = True\n",
    "\n",
    "model.load_state_dict(sd, strict=False)\n",
    "\n",
    "print(\"Loaded DET warm-start from:\", CKPT_DET_PATH)\n",
    "print(\"Note: logsigma_head is NEW and will be trained.\")\n",
    "\n",
    "# Freeze/unfreeze helpers\n",
    "def set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "def freeze_mean_parts(m):\n",
    "    set_requires_grad(m.city_emb, False)\n",
    "    set_requires_grad(m.backbone, False)\n",
    "    set_requires_grad(m.mean_head, False)\n",
    "    m.D_raw.requires_grad = False\n",
    "    m.k_raw.requires_grad = False\n",
    "    # sigma head trainable\n",
    "    set_requires_grad(m.logsig_head, True)\n",
    "\n",
    "def unfreeze_all(m):\n",
    "    set_requires_grad(m, True)\n",
    "\n",
    "# Training loop\n",
    "@torch.no_grad()\n",
    "def eval_val(idx):\n",
    "    model.eval()\n",
    "    losses, viols = [], []\n",
    "    for b0 in range(0, len(idx), BATCH_TIMES):\n",
    "        ii = idx[b0:b0+BATCH_TIMES]\n",
    "        c0b   = torch.from_numpy(c0[ii]).to(DEVICE)\n",
    "        Xb    = torch.from_numpy(X[ii]).to(DEVICE)\n",
    "        yb    = torch.from_numpy(y[ii]).to(DEVICE)\n",
    "        pm10b = torch.from_numpy(pm10_tgt[ii]).to(DEVICE)\n",
    "\n",
    "        mu, sigma = model(c0b, Xb, H)\n",
    "\n",
    "        nll = gaussian_nll(mu, sigma, yb).mean()\n",
    "        loss = nll\n",
    "        loss = loss + LAMBDA_INEQ * hinge_pos(mu - pm10b).mean()\n",
    "        loss = loss + LAMBDA_NONNEG * hinge_pos(-mu).mean()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        viols.append(((mu - pm10b) > 0).float().mean().item())\n",
    "    return float(np.mean(losses)), float(np.mean(viols))\n",
    "\n",
    "best_val = 1e18\n",
    "best_state = None\n",
    "bad = 0\n",
    "\n",
    "for ep in range(1, EPOCHS_TOTAL+1):\n",
    "    if ep <= FREEZE_MEAN_EPOCHS:\n",
    "        freeze_mean_parts(model)\n",
    "        lr = LR_SIGMA\n",
    "    else:\n",
    "        unfreeze_all(model)\n",
    "        lr = LR_FINE\n",
    "\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                           lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    model.train()\n",
    "    np.random.shuffle(tr_idx)\n",
    "    tr_losses = []\n",
    "\n",
    "    for b0 in range(0, len(tr_idx), BATCH_TIMES):\n",
    "        ii = tr_idx[b0:b0+BATCH_TIMES]\n",
    "        c0b   = torch.from_numpy(c0[ii]).to(DEVICE)\n",
    "        Xb    = torch.from_numpy(X[ii]).to(DEVICE)\n",
    "        yb    = torch.from_numpy(y[ii]).to(DEVICE)\n",
    "        pm10b = torch.from_numpy(pm10_tgt[ii]).to(DEVICE)\n",
    "\n",
    "        mu, sigma = model(c0b, Xb, H)\n",
    "\n",
    "        nll = gaussian_nll(mu, sigma, yb).mean()\n",
    "\n",
    "        loss = nll\n",
    "        loss = loss + LAMBDA_INEQ * hinge_pos(mu - pm10b).mean()\n",
    "        loss = loss + LAMBDA_NONNEG * hinge_pos(-mu).mean()\n",
    "\n",
    "        # mild regularizer to avoid sigma exploding early\n",
    "        loss = loss + 1e-4 * (sigma.mean())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    val_loss, val_viol = eval_val(va_idx)\n",
    "    print(f\"[UQ h=24] ep {ep:02d} | train {np.mean(tr_losses):.4f} | val_loss {val_loss:.4f} | \"\n",
    "          f\"val_viol {100*val_viol:.2f}% | D {model.D().item():.4f} k {model.k().item():.4f} | lr {lr:g}\")\n",
    "\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= PATIENCE:\n",
    "            print(f\"Early stop. Best val_loss={best_val:.4f}\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "torch.save({\"state_dict\": model.state_dict()}, CKPT_UQ_PATH)\n",
    "print(\"Saved UQ checkpoint:\", CKPT_UQ_PATH)\n",
    "\n",
    "# MC inference => mu_mean, sigma components, exceedance probability\n",
    "@torch.no_grad()\n",
    "def mc_predict(idx, split_name):\n",
    "    model.eval()\n",
    "    enable_dropout(model)  # MC-dropout ON\n",
    "\n",
    "    Tn = len(idx)\n",
    "    mu_mean = np.zeros((Tn, N), dtype=np.float32)\n",
    "    p_raw   = np.zeros((Tn, N), dtype=np.float32)\n",
    "    epi_var = np.zeros((Tn, N), dtype=np.float32)\n",
    "    ale_var = np.zeros((Tn, N), dtype=np.float32)\n",
    "\n",
    "    y_true = y[idx,:,0].astype(np.float32)\n",
    "    pm10_true = pm10_tgt[idx,:,0].astype(np.float32)\n",
    "\n",
    "    for b0 in range(0, Tn, BATCH_TIMES):\n",
    "        sl = slice(b0, min(b0+BATCH_TIMES, Tn))\n",
    "        ii = idx[sl]\n",
    "        c0b = torch.from_numpy(c0[ii]).to(DEVICE)\n",
    "        Xb  = torch.from_numpy(X[ii]).to(DEVICE)\n",
    "\n",
    "        mu_sum   = torch.zeros((len(ii), N, 1), device=DEVICE)\n",
    "        mu2_sum  = torch.zeros((len(ii), N, 1), device=DEVICE)\n",
    "        sig2_sum = torch.zeros((len(ii), N, 1), device=DEVICE)\n",
    "        p_sum    = torch.zeros((len(ii), N, 1), device=DEVICE)\n",
    "\n",
    "        T_thr = torch.tensor(T_EXCEED, device=DEVICE).view(1,1,1)\n",
    "\n",
    "        for _ in range(M_MC):\n",
    "            mu_m, sig_m = model(c0b, Xb, H)  # (B,N,1)\n",
    "            mu_sum  += mu_m\n",
    "            mu2_sum += mu_m**2\n",
    "            sig2_sum += sig_m**2\n",
    "\n",
    "            z = (T_thr - mu_m) / sig_m\n",
    "            p_m = 1.0 - normal_cdf(z)\n",
    "            p_sum += p_m\n",
    "\n",
    "        mu_bar = mu_sum / M_MC\n",
    "        mu_var = (mu2_sum / M_MC - mu_bar**2).clamp_min(0.0)  # epistemic variance of mean\n",
    "        ale    = sig2_sum / M_MC                              # aleatoric variance\n",
    "\n",
    "        pbar = (p_sum / M_MC).clamp(0.0, 1.0)\n",
    "\n",
    "        mu_mean[sl,:] = mu_bar.squeeze(-1).cpu().numpy()\n",
    "        p_raw[sl,:]   = pbar.squeeze(-1).cpu().numpy()\n",
    "        epi_var[sl,:] = mu_var.squeeze(-1).cpu().numpy()\n",
    "        ale_var[sl,:] = ale.squeeze(-1).cpu().numpy()\n",
    "\n",
    "    rows = []\n",
    "    for local_ti, global_ti in enumerate(idx):\n",
    "        t = times[global_ti]  # issuance time\n",
    "        for n in range(N):\n",
    "            rows.append({\n",
    "                \"city_id\": int(city_ids[n]),\n",
    "                \"datetime\": pd.Timestamp(t),\n",
    "                \"split\": split_name,\n",
    "                \"horizon_h\": H,\n",
    "                \"y_true\": float(y_true[local_ti, n]),\n",
    "                \"pm10_true\": float(pm10_true[local_ti, n]),\n",
    "                \"mu_mean\": float(mu_mean[local_ti, n]),\n",
    "                \"p_exceed_raw\": float(np.clip(p_raw[local_ti, n], 0.0, 1.0)),\n",
    "                \"sigma_epi\": float(math.sqrt(max(epi_var[local_ti, n], 0.0))),\n",
    "                \"sigma_ale\": float(math.sqrt(max(ale_var[local_ti, n], 0.0))),\n",
    "                \"sigma_total\": float(math.sqrt(max(epi_var[local_ti, n] + ale_var[local_ti, n], 0.0))),\n",
    "                \"z_true\": int(y_true[local_ti, n] > T_EXCEED),\n",
    "                # optional: keep lat/lon for maps\n",
    "                \"lat\": float(city_meta.loc[n, \"lat\"]),\n",
    "                \"lon\": float(city_meta.loc[n, \"lon\"]),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\nMC inference on VAL...\")\n",
    "df_val = mc_predict(va_idx, \"val\")\n",
    "print(\"VAL rows:\", len(df_val))\n",
    "\n",
    "print(\"\\nMC inference on TEST...\")\n",
    "df_test = mc_predict(te_idx, \"test\")\n",
    "print(\"TEST rows:\", len(df_test))\n",
    "\n",
    "# Calibration (isotonic on VAL) + metrics\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(df_val[\"p_exceed_raw\"].values, df_val[\"z_true\"].values)\n",
    "\n",
    "df_val[\"p_exceed_cal\"]  = iso.transform(df_val[\"p_exceed_raw\"].values)\n",
    "df_test[\"p_exceed_cal\"] = iso.transform(df_test[\"p_exceed_raw\"].values)\n",
    "\n",
    "# Save predictions\n",
    "pred_all = pd.concat([df_val, df_test], ignore_index=True)\n",
    "pred_all.to_csv(OUT_PRED_UQ, index=False)\n",
    "print(\"\\nSaved:\", OUT_PRED_UQ)\n",
    "\n",
    "# Choose tau_star on VAL (maximize F1) and report both policy tau and tau_star\n",
    "tau_star, f1_star = pick_tau_by_val(df_val, \"p_exceed_cal\")\n",
    "print(f\"tau_star (VAL max-F1) = {tau_star:.2f} (F1={f1_star:.3f}) | tau_policy={TAU_POLICY:.2f}\")\n",
    "\n",
    "met_rows = []\n",
    "for split_name, d in [(\"val\", df_val), (\"test\", df_test)]:\n",
    "    for pcol in [\"p_exceed_raw\", \"p_exceed_cal\"]:\n",
    "        # metrics at tau_policy and tau_star\n",
    "        for tau in [TAU_POLICY, tau_star]:\n",
    "            m = compute_metrics(d, pcol, tau)\n",
    "            m.update({\n",
    "                \"split\": split_name,\n",
    "                \"T_exceed\": float(T_EXCEED),\n",
    "                \"tau_policy\": float(TAU_POLICY),\n",
    "                \"tau_star_val\": float(tau_star),\n",
    "            })\n",
    "            met_rows.append(m)\n",
    "\n",
    "met = pd.DataFrame(met_rows).sort_values([\"split\",\"p_col\",\"tau\"]).reset_index(drop=True)\n",
    "met.to_csv(OUT_MET_UQ, index=False)\n",
    "print(\"Saved:\", OUT_MET_UQ)\n",
    "print(met)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763be34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
