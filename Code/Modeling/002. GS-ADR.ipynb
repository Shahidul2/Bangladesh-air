{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4175327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Cities: 29  Graph k=4  Avg deg=4.90\n",
      "[h= 1] ep 01 | train 4.1647 | val_MAE 3.8813 | val_viol 16.47% | D 0.5864 k 0.3062\n",
      "[h= 1] ep 02 | train 2.7254 | val_MAE 3.5433 | val_viol 15.28% | D 0.5375 k 0.3049\n",
      "[h= 1] ep 03 | train 2.5602 | val_MAE 3.4039 | val_viol 17.17% | D 0.4978 k 0.3026\n",
      "[h= 1] ep 04 | train 2.4651 | val_MAE 3.3828 | val_viol 15.44% | D 0.4656 k 0.2997\n",
      "[h= 1] ep 05 | train 2.3890 | val_MAE 3.2875 | val_viol 16.64% | D 0.4406 k 0.2961\n",
      "[h= 1] ep 06 | train 2.3586 | val_MAE 3.2765 | val_viol 16.40% | D 0.4171 k 0.2922\n",
      "[h= 1] ep 07 | train 2.3143 | val_MAE 3.3081 | val_viol 13.21% | D 0.3967 k 0.2879\n",
      "[h= 1] ep 08 | train 2.2907 | val_MAE 3.1779 | val_viol 15.34% | D 0.3777 k 0.2831\n",
      "[h= 1] ep 09 | train 2.2697 | val_MAE 3.1110 | val_viol 14.47% | D 0.3585 k 0.2782\n",
      "[h= 1] ep 10 | train 2.2537 | val_MAE 3.1796 | val_viol 13.13% | D 0.3409 k 0.2733\n",
      "[h= 1] ep 11 | train 2.2136 | val_MAE 3.1453 | val_viol 12.84% | D 0.3275 k 0.2682\n",
      "[h= 1] ep 12 | train 2.1890 | val_MAE 3.1181 | val_viol 14.00% | D 0.3156 k 0.2631\n",
      "[h= 1] ep 13 | train 2.1842 | val_MAE 3.0379 | val_viol 13.37% | D 0.3017 k 0.2580\n",
      "[h= 1] ep 14 | train 2.1710 | val_MAE 3.0615 | val_viol 15.55% | D 0.2887 k 0.2529\n",
      "[h= 1] ep 15 | train 2.1485 | val_MAE 3.1513 | val_viol 12.16% | D 0.2783 k 0.2480\n",
      "[h= 1] ep 16 | train 2.1421 | val_MAE 3.0445 | val_viol 14.03% | D 0.2682 k 0.2429\n",
      "[h= 1] ep 17 | train 2.1171 | val_MAE 3.0929 | val_viol 11.80% | D 0.2591 k 0.2381\n",
      "[h= 1] ep 18 | train 2.1158 | val_MAE 2.9723 | val_viol 14.61% | D 0.2501 k 0.2333\n",
      "[h= 1] ep 19 | train 2.0974 | val_MAE 2.9755 | val_viol 12.55% | D 0.2425 k 0.2286\n",
      "[h= 1] ep 20 | train 2.0805 | val_MAE 3.0193 | val_viol 11.61% | D 0.2355 k 0.2239\n",
      "[h= 1] ep 21 | train 2.0763 | val_MAE 3.0448 | val_viol 11.20% | D 0.2284 k 0.2194\n",
      "[h= 1] ep 22 | train 2.0655 | val_MAE 2.9424 | val_viol 16.24% | D 0.2216 k 0.2149\n",
      "[h= 1] ep 23 | train 2.0557 | val_MAE 2.9467 | val_viol 12.08% | D 0.2161 k 0.2107\n",
      "[h= 1] ep 24 | train 2.0479 | val_MAE 2.9570 | val_viol 12.97% | D 0.2114 k 0.2064\n",
      "[h= 1] ep 25 | train 2.0298 | val_MAE 2.9071 | val_viol 12.79% | D 0.2072 k 0.2021\n",
      "[h= 1] ep 26 | train 2.0288 | val_MAE 3.1023 | val_viol 10.08% | D 0.2019 k 0.1980\n",
      "[h= 1] ep 27 | train 2.0307 | val_MAE 2.9717 | val_viol 15.24% | D 0.1964 k 0.1940\n",
      "[h= 1] ep 28 | train 2.0130 | val_MAE 2.8856 | val_viol 12.89% | D 0.1929 k 0.1901\n",
      "[h= 1] ep 29 | train 2.0026 | val_MAE 2.8579 | val_viol 12.01% | D 0.1888 k 0.1863\n",
      "[h= 1] ep 30 | train 1.9989 | val_MAE 2.8596 | val_viol 13.39% | D 0.1851 k 0.1825\n",
      "[h= 1] TEST | MAE 1.9561 | viol 19.50%\n",
      "[h= 6] ep 01 | train 12.1754 | val_MAE 13.8698 | val_viol 31.17% | D 0.5651 k 0.3093\n",
      "[h= 6] ep 02 | train 9.4463 | val_MAE 12.9321 | val_viol 26.51% | D 0.5209 k 0.3081\n",
      "[h= 6] ep 03 | train 9.0124 | val_MAE 12.6683 | val_viol 29.73% | D 0.5054 k 0.3065\n",
      "[h= 6] ep 04 | train 8.8025 | val_MAE 12.4938 | val_viol 26.20% | D 0.4948 k 0.3044\n",
      "[h= 6] ep 05 | train 8.6918 | val_MAE 12.2518 | val_viol 27.41% | D 0.4809 k 0.3027\n",
      "[h= 6] ep 06 | train 8.5847 | val_MAE 11.9126 | val_viol 29.86% | D 0.4713 k 0.3010\n",
      "[h= 6] ep 07 | train 8.4903 | val_MAE 12.3091 | val_viol 27.47% | D 0.4599 k 0.2994\n",
      "[h= 6] ep 08 | train 8.4088 | val_MAE 11.8985 | val_viol 30.78% | D 0.4534 k 0.2976\n",
      "[h= 6] ep 09 | train 8.3724 | val_MAE 12.2216 | val_viol 24.18% | D 0.4392 k 0.2960\n",
      "[h= 6] ep 10 | train 8.2746 | val_MAE 11.9516 | val_viol 24.30% | D 0.4381 k 0.2941\n",
      "[h= 6] ep 11 | train 8.2345 | val_MAE 11.9682 | val_viol 29.69% | D 0.4343 k 0.2921\n",
      "[h= 6] ep 12 | train 8.1281 | val_MAE 11.8279 | val_viol 26.20% | D 0.4382 k 0.2895\n",
      "[h= 6] ep 13 | train 8.1602 | val_MAE 11.6690 | val_viol 26.59% | D 0.4304 k 0.2876\n",
      "[h= 6] ep 14 | train 8.0608 | val_MAE 11.9053 | val_viol 25.34% | D 0.4344 k 0.2854\n",
      "[h= 6] ep 15 | train 8.0720 | val_MAE 11.7797 | val_viol 33.67% | D 0.4309 k 0.2833\n",
      "[h= 6] ep 16 | train 8.0281 | val_MAE 12.0245 | val_viol 25.84% | D 0.4273 k 0.2815\n",
      "[h= 6] ep 17 | train 8.0269 | val_MAE 12.3081 | val_viol 21.70% | D 0.4219 k 0.2800\n",
      "[h= 6] ep 18 | train 7.9648 | val_MAE 11.4048 | val_viol 29.82% | D 0.4199 k 0.2775\n",
      "[h= 6] ep 19 | train 7.9243 | val_MAE 11.2949 | val_viol 34.39% | D 0.4251 k 0.2752\n",
      "[h= 6] ep 20 | train 7.8748 | val_MAE 11.6038 | val_viol 25.17% | D 0.4286 k 0.2733\n",
      "[h= 6] ep 21 | train 7.9245 | val_MAE 11.5599 | val_viol 27.99% | D 0.4173 k 0.2717\n",
      "[h= 6] ep 22 | train 7.8233 | val_MAE 11.6161 | val_viol 27.34% | D 0.4209 k 0.2695\n",
      "[h= 6] ep 23 | train 7.8060 | val_MAE 11.5495 | val_viol 28.61% | D 0.4217 k 0.2674\n",
      "[h= 6] ep 24 | train 7.8037 | val_MAE 11.5646 | val_viol 23.99% | D 0.4198 k 0.2656\n",
      "[h=6] Early stop. Best val_MAE=11.2949\n",
      "[h= 6] TEST | MAE 7.3051 | viol 34.77%\n",
      "[h=24] ep 01 | train 13.7564 | val_MAE 14.2711 | val_viol 30.50% | D 0.5617 k 0.3107\n",
      "[h=24] ep 02 | train 11.7144 | val_MAE 13.8651 | val_viol 34.72% | D 0.4770 k 0.3122\n",
      "[h=24] ep 03 | train 11.3827 | val_MAE 13.8656 | val_viol 30.52% | D 0.4632 k 0.3125\n",
      "[h=24] ep 04 | train 11.2440 | val_MAE 15.3022 | val_viol 21.72% | D 0.4670 k 0.3121\n",
      "[h=24] ep 05 | train 11.1796 | val_MAE 14.6801 | val_viol 24.93% | D 0.4796 k 0.3113\n",
      "[h=24] ep 06 | train 11.1450 | val_MAE 13.7834 | val_viol 33.02% | D 0.4853 k 0.3105\n",
      "[h=24] ep 07 | train 11.0212 | val_MAE 13.7214 | val_viol 31.61% | D 0.5085 k 0.3094\n",
      "[h=24] ep 08 | train 10.9640 | val_MAE 13.7440 | val_viol 33.98% | D 0.5294 k 0.3081\n",
      "[h=24] ep 09 | train 10.9975 | val_MAE 14.2079 | val_viol 25.82% | D 0.5397 k 0.3079\n",
      "[h=24] ep 10 | train 10.8941 | val_MAE 13.8066 | val_viol 35.42% | D 0.5569 k 0.3067\n",
      "[h=24] ep 11 | train 10.9472 | val_MAE 13.9005 | val_viol 33.54% | D 0.5627 k 0.3064\n",
      "[h=24] ep 12 | train 10.8760 | val_MAE 13.9372 | val_viol 28.15% | D 0.5758 k 0.3058\n",
      "[h=24] Early stop. Best val_MAE=13.7214\n",
      "[h=24] TEST | MAE 9.4734 | viol 30.89%\n",
      "\n",
      "Saved: predictions_pignnpp.csv  rows=680688  models=1\n",
      "Saved: predictions_all_with_pignnpp.csv  rows=4084128\n"
     ]
    }
   ],
   "source": [
    "# GS-ADR for PM2.5 forecasting \n",
    "#  1) Implicit macro-step dynamics:\n",
    "#       (I + hdt*(D*L + kI)) c_{t+h} = c_t + hdt*s_theta(...)\n",
    "#  2) Target-time cyclical features (hour/doy at t+h)\n",
    "#  3) Graph-aware source term: uses neighbor-aggregated features and neighbor PM2.5\n",
    "#  5) City embeddings\n",
    "#  6) Train-only scaling, strict time split, saves predictions in baseline schema\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "DATA_PATH = \"dataset_2023_2025.csv\"\n",
    "BASELINE_PRED_PATH = \"predictions_baselines.csv\"  \n",
    "OUT_PRED_PATH = \"predictions_pignnpp.csv\"\n",
    "OUT_ALL_PATH = \"predictions_all_with_pignnpp.csv\"\n",
    "\n",
    "K_GRAPH = 4\n",
    "ELL_KM = 120.0\n",
    "\n",
    "HORIZONS = [1, 6, 24]\n",
    "PM_LAGS = [1, 2, 6, 24]\n",
    "CHEMS = ['carbon_monoxide', 'nitrogen_dioxide', 'sulphur_dioxide', 'ozone']\n",
    "CHEM_LAGS = [1, 6, 24]\n",
    "\n",
    "TRAIN_END = \"2024-12-31 23:00:00\"\n",
    "VAL_END   = \"2025-06-30 23:00:00\"\n",
    "TEST_END  = \"2025-11-23 23:00:00\"\n",
    "\n",
    "# training\n",
    "SEED = 42\n",
    "BATCH_TIMES = 128\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 1e-6\n",
    "\n",
    "# physics + constraints\n",
    "DT = 1.0\n",
    "LAMBDA_INEQ = 1.0      \n",
    "LAMBDA_NONNEG = 0.05\n",
    "\n",
    "# model capacity\n",
    "DROPOUT = 0.20\n",
    "EMB_DIM = 8\n",
    "HIDDEN = 96\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def time_split_times(times):\n",
    "    times = pd.to_datetime(times)\n",
    "    tr = times[times <= pd.Timestamp(TRAIN_END)]\n",
    "    va = times[(times > pd.Timestamp(TRAIN_END)) & (times <= pd.Timestamp(VAL_END))]\n",
    "    te = times[(times > pd.Timestamp(VAL_END)) & (times <= pd.Timestamp(TEST_END))]\n",
    "    return tr, va, te\n",
    "\n",
    "def build_knn_graph(city_meta, k=4, ell_km=120.0):\n",
    "    coords = city_meta[['lat', 'lon']].to_numpy()\n",
    "    coords_rad = np.radians(coords)\n",
    "\n",
    "    nnm = NearestNeighbors(n_neighbors=k+1, metric='haversine')\n",
    "    nnm.fit(coords_rad)\n",
    "    dists, idxs = nnm.kneighbors(coords_rad)\n",
    "\n",
    "    n = len(city_meta)\n",
    "    W = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        for t in range(1, k+1):\n",
    "            j = idxs[i, t]\n",
    "            dist_km = dists[i, t] * 6371.0\n",
    "            w = np.exp(-(dist_km / ell_km) ** 2)\n",
    "            if w > W[i, j]:\n",
    "                W[i, j] = w\n",
    "            if w > W[j, i]:\n",
    "                W[j, i] = w\n",
    "\n",
    "    D = np.diag(W.sum(axis=1))\n",
    "    L = (D - W).astype(np.float32)\n",
    "    return W, L\n",
    "\n",
    "def hinge_pos(x):\n",
    "    return F.relu(x)\n",
    "\n",
    "def mae_loss(pred, y):\n",
    "    return torch.mean(torch.abs(pred - y))\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "keep_cols = [\n",
    "    'city_id','city_name','lat','lon','datetime',\n",
    "    'pm2_5','pm10',\n",
    "] + CHEMS\n",
    "df = df[keep_cols].copy()\n",
    "df = df.sort_values(['city_id','datetime']).reset_index(drop=True)\n",
    "\n",
    "# Build lags \n",
    "for lag in PM_LAGS:\n",
    "    df[f'pm2_5_lag{lag}'] = df.groupby('city_id')['pm2_5'].shift(lag)\n",
    "\n",
    "for col in CHEMS:\n",
    "    for lag in CHEM_LAGS:\n",
    "        df[f'{col}_lag{lag}'] = df.groupby('city_id')[col].shift(lag)\n",
    "\n",
    "# Targets for each horizon\n",
    "for h in HORIZONS:\n",
    "    df[f'y_h{h}'] = df.groupby('city_id')['pm2_5'].shift(-h)\n",
    "    df[f'pm10_h{h}'] = df.groupby('city_id')['pm10'].shift(-h)\n",
    "\n",
    "# Drop NA rows needed for any horizon (keeps things consistent)\n",
    "needed = []\n",
    "needed += [f'pm2_5_lag{l}' for l in PM_LAGS]\n",
    "needed += [f'{c}_lag{l}' for c in CHEMS for l in CHEM_LAGS]\n",
    "needed += [f'y_h{h}' for h in HORIZONS] + [f'pm10_h{h}' for h in HORIZONS]\n",
    "df = df.dropna(subset=needed).copy()\n",
    "\n",
    "# City metadata + ordering\n",
    "city_meta = df[['city_id','city_name','lat','lon']].drop_duplicates().sort_values('city_id').reset_index(drop=True)\n",
    "city_ids = city_meta['city_id'].to_numpy()\n",
    "cid_to_idx = {cid:i for i,cid in enumerate(city_ids)}\n",
    "N = len(city_ids)\n",
    "\n",
    "# Graph\n",
    "W_np, L_np = build_knn_graph(city_meta, k=K_GRAPH, ell_km=ELL_KM)\n",
    "L = torch.tensor(L_np, device=DEVICE)  # (N,N)\n",
    "W = torch.tensor(W_np, device=DEVICE)  # (N,N)\n",
    "\n",
    "# Row-normalized adjacency for neighbor aggregation\n",
    "deg = W.sum(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "A_norm = W / deg  # (N,N)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Cities: {N}  Graph k={K_GRAPH}  Avg deg={(W_np>0).sum(axis=1).mean():.2f}\")\n",
    "\n",
    "# Build time-indexed tensors (T,N,F) per horizon\n",
    "def build_time_tensor(df_in, horizon_h):\n",
    "    df2 = df_in.copy()\n",
    "    df2['cid_idx'] = df2['city_id'].map(cid_to_idx)\n",
    "    df2 = df2.sort_values(['datetime','cid_idx'])\n",
    "\n",
    "    # target-time cyclical features (t+h)\n",
    "    dt_tgt = df2['datetime'] + pd.to_timedelta(horizon_h, unit='h')\n",
    "    hour = dt_tgt.dt.hour.values\n",
    "    doy = dt_tgt.dt.dayofyear.values\n",
    "\n",
    "    df2['hour_sin_tgt'] = np.sin(2*np.pi*hour/24.0)\n",
    "    df2['hour_cos_tgt'] = np.cos(2*np.pi*hour/24.0)\n",
    "    df2['doy_sin_tgt']  = np.sin(2*np.pi*doy/365.0)\n",
    "    df2['doy_cos_tgt']  = np.cos(2*np.pi*doy/365.0)\n",
    "\n",
    "    # feature columns (PM10 excluded)\n",
    "    X_COLS = []\n",
    "    # current chems\n",
    "    X_COLS += CHEMS\n",
    "    # chemical lags\n",
    "    X_COLS += [f'{c}_lag{l}' for c in CHEMS for l in CHEM_LAGS]\n",
    "    # PM2.5 lags\n",
    "    X_COLS += [f'pm2_5_lag{l}' for l in PM_LAGS]\n",
    "    # target-time calendar\n",
    "    X_COLS += ['doy_sin_tgt','doy_cos_tgt','hour_sin_tgt','hour_cos_tgt']\n",
    "    # static\n",
    "    X_COLS += ['lat','lon']\n",
    "\n",
    "    # keep only full times (all cities present)\n",
    "    counts = df2.groupby('datetime')['cid_idx'].nunique()\n",
    "    full_times = counts[counts == N].index\n",
    "    df2 = df2[df2['datetime'].isin(full_times)].copy()\n",
    "\n",
    "    times = pd.to_datetime(np.sort(df2['datetime'].unique()))\n",
    "    T = len(times)\n",
    "\n",
    "    X = np.zeros((T, N, len(X_COLS)), dtype=np.float32)\n",
    "    c0 = np.zeros((T, N, 1), dtype=np.float32)\n",
    "    y  = np.zeros((T, N, 1), dtype=np.float32)\n",
    "    pm10_tgt = np.zeros((T, N, 1), dtype=np.float32)\n",
    "\n",
    "    g = df2.groupby('datetime', sort=True)\n",
    "    for ti, t in enumerate(times):\n",
    "        gt = g.get_group(t).sort_values('cid_idx')\n",
    "        X[ti,:,:] = gt[X_COLS].to_numpy(np.float32)\n",
    "        c0[ti,:,0] = gt['pm2_5'].to_numpy(np.float32)\n",
    "        y[ti,:,0]  = gt[f'y_h{horizon_h}'].to_numpy(np.float32)\n",
    "        pm10_tgt[ti,:,0] = gt[f'pm10_h{horizon_h}'].to_numpy(np.float32)\n",
    "\n",
    "    return times, X, c0, y, pm10_tgt, X_COLS\n",
    "\n",
    "# Model: Graph-aware source + city embeddings + implicit macro-step physics\n",
    "class SourceNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=96, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):   # (B,N,in_dim)\n",
    "        return self.net(z)  # (B,N,1)\n",
    "\n",
    "class PIGNNPP(nn.Module):\n",
    "    def __init__(self, x_dim, L, A_norm, n_cities, emb_dim=8, hidden=96, dt=1.0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.A = A_norm\n",
    "        self.dt = dt\n",
    "\n",
    "        self.city_emb = nn.Embedding(n_cities, emb_dim)\n",
    "        self.register_buffer(\"city_idx\", torch.arange(n_cities, dtype=torch.long))\n",
    "\n",
    "        # Source gets: [x, x_nb, c0, c_nb, emb]\n",
    "        self.source = SourceNet(in_dim=2*x_dim + 2*1 + emb_dim, hidden=hidden, dropout=dropout)\n",
    "\n",
    "        self.D_raw = nn.Parameter(torch.tensor(0.0))\n",
    "        self.k_raw = nn.Parameter(torch.tensor(-1.0))\n",
    "\n",
    "    def D(self):\n",
    "        return F.softplus(self.D_raw) + 1e-6\n",
    "\n",
    "    def k(self):\n",
    "        return F.softplus(self.k_raw) + 1e-6\n",
    "\n",
    "    def forward(self, c0, x, steps):\n",
    "        \"\"\"\n",
    "        c0: (B,N,1)\n",
    "        x : (B,N,F)\n",
    "        \"\"\"\n",
    "        # neighbor-aggregated features/state\n",
    "        x_nb = torch.einsum('ij,bjk->bik', self.A, x)      # (B,N,F)\n",
    "        c_nb = torch.einsum('ij,bjk->bik', self.A, c0)     # (B,N,1)\n",
    "\n",
    "        # city embeddings (N,emb) -> (B,N,emb)\n",
    "        emb = self.city_emb(self.city_idx).unsqueeze(0).expand(x.shape[0], -1, -1)\n",
    "\n",
    "        # source input\n",
    "        z = torch.cat([x, x_nb, c0, c_nb, emb], dim=-1)    # (B,N, 2F+2+emb)\n",
    "        s = self.source(z)                                 # (B,N,1)\n",
    "\n",
    "        # physics implicit macro-step\n",
    "        D = self.D()\n",
    "        k = self.k()\n",
    "        hdt = steps * self.dt\n",
    "\n",
    "        N = self.L.shape[0]\n",
    "        I = torch.eye(N, device=c0.device, dtype=c0.dtype)\n",
    "\n",
    "        A_sys = I + hdt * (D * self.L + k * I)            # (N,N)\n",
    "        A_b = A_sys.unsqueeze(0).expand(c0.shape[0], -1, -1)\n",
    "\n",
    "        rhs = c0 + hdt * s                                # (B,N,1)\n",
    "        pred = torch.linalg.solve(A_b, rhs)\n",
    "        return pred\n",
    "\n",
    "# Evaluation\n",
    "@torch.no_grad()\n",
    "def eval_mae(model, c0, X, y, pm10_tgt, times_idx, steps):\n",
    "    model.eval()\n",
    "    losses, viols = [], []\n",
    "\n",
    "    for b0 in range(0, len(times_idx), BATCH_TIMES):\n",
    "        idx = times_idx[b0:b0+BATCH_TIMES]\n",
    "        c0b = torch.from_numpy(c0[idx]).to(DEVICE)\n",
    "        Xb  = torch.from_numpy(X[idx]).to(DEVICE)\n",
    "        yb  = torch.from_numpy(y[idx]).to(DEVICE)\n",
    "        pm10b = torch.from_numpy(pm10_tgt[idx]).to(DEVICE)\n",
    "\n",
    "        pred = model(c0b, Xb, steps)\n",
    "        losses.append(torch.mean(torch.abs(pred - yb)).item())\n",
    "\n",
    "        v = (pred - pm10b) > 0\n",
    "        viols.append(v.float().mean().item())\n",
    "\n",
    "    return float(np.mean(losses)), float(np.mean(viols))\n",
    "\n",
    "# Train one horizon\n",
    "def train_one_horizon(h):\n",
    "    times, X, c0, y, pm10_tgt, X_COLS = build_time_tensor(df, h)\n",
    "    tr_times, va_times, te_times = time_split_times(times)\n",
    "\n",
    "    time_to_i = {t:i for i,t in enumerate(times)}\n",
    "    tr_idx = np.array([time_to_i[t] for t in tr_times], dtype=int)\n",
    "    va_idx = np.array([time_to_i[t] for t in va_times], dtype=int)\n",
    "    te_idx = np.array([time_to_i[t] for t in te_times], dtype=int)\n",
    "\n",
    "    # scale X using TRAIN only\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[tr_idx].reshape(-1, X.shape[-1]))\n",
    "    X = scaler.transform(X.reshape(-1, X.shape[-1])).astype(np.float32).reshape(X.shape)\n",
    "\n",
    "    model = PIGNNPP(\n",
    "        x_dim=X.shape[-1],\n",
    "        L=L,\n",
    "        A_norm=A_norm,\n",
    "        n_cities=N,\n",
    "        emb_dim=EMB_DIM,\n",
    "        hidden=HIDDEN,\n",
    "        dt=DT,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 1e18\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "    steps = int(h)\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        np.random.shuffle(tr_idx)\n",
    "\n",
    "        tr_losses = []\n",
    "        for b0 in range(0, len(tr_idx), BATCH_TIMES):\n",
    "            idx = tr_idx[b0:b0+BATCH_TIMES]\n",
    "            c0b = torch.from_numpy(c0[idx]).to(DEVICE)\n",
    "            Xb  = torch.from_numpy(X[idx]).to(DEVICE)\n",
    "            yb  = torch.from_numpy(y[idx]).to(DEVICE)\n",
    "            pm10b = torch.from_numpy(pm10_tgt[idx]).to(DEVICE)\n",
    "\n",
    "            pred = model(c0b, Xb, steps)\n",
    "            loss = mae_loss(pred, yb)\n",
    "\n",
    "            # constraints (train only)\n",
    "            if LAMBDA_INEQ > 0:\n",
    "                loss = loss + LAMBDA_INEQ * torch.mean(hinge_pos(pred - pm10b))\n",
    "            if LAMBDA_NONNEG > 0:\n",
    "                loss = loss + LAMBDA_NONNEG * torch.mean(hinge_pos(-pred))\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            tr_losses.append(loss.item())\n",
    "\n",
    "        val_mae, val_viol = eval_mae(model, c0, X, y, pm10_tgt, va_idx, steps)\n",
    "        D_val = model.D().item()\n",
    "        k_val = model.k().item()\n",
    "\n",
    "        print(f\"[h={h:2d}] ep {ep:02d} | train {np.mean(tr_losses):.4f} | val_MAE {val_mae:.4f} | \"\n",
    "              f\"val_viol {100*val_viol:.2f}% | D {D_val:.4f} k {k_val:.4f}\")\n",
    "\n",
    "        if val_mae < best_val - 1e-4:\n",
    "            best_val = val_mae\n",
    "            best_state = {kk: vv.detach().cpu().clone() for kk, vv in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= PATIENCE:\n",
    "                print(f\"[h={h}] Early stop. Best val_MAE={best_val:.4f}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    test_mae, test_viol = eval_mae(model, c0, X, y, pm10_tgt, te_idx, steps)\n",
    "    print(f\"[h={h:2d}] TEST | MAE {test_mae:.4f} | viol {100*test_viol:.2f}%\")\n",
    "\n",
    "    return model, scaler, times, X, c0, y, pm10_tgt, va_idx, te_idx\n",
    "\n",
    "# Predict + save\n",
    "@torch.no_grad()\n",
    "def predict_split(model, times, X, c0, y, pm10_tgt, idx, h, split_name, model_name):\n",
    "    model.eval()\n",
    "    steps = int(h)\n",
    "    out_rows = []\n",
    "\n",
    "    for b0 in range(0, len(idx), BATCH_TIMES):\n",
    "        ii = idx[b0:b0+BATCH_TIMES]\n",
    "        c0b = torch.from_numpy(c0[ii]).to(DEVICE)\n",
    "        Xb  = torch.from_numpy(X[ii]).to(DEVICE)\n",
    "\n",
    "        pred = model(c0b, Xb, steps).detach().cpu().numpy()  # (B,N,1)\n",
    "\n",
    "        for bi, ti in enumerate(ii):\n",
    "            t = times[ti]\n",
    "            for n in range(N):\n",
    "                out_rows.append({\n",
    "                    \"city_id\": int(city_ids[n]),\n",
    "                    \"datetime\": pd.Timestamp(t),\n",
    "                    \"split\": split_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"horizon_h\": int(h),\n",
    "                    \"y_true\": float(y[ti, n, 0]),\n",
    "                    \"y_pred\": float(pred[bi, n, 0]),\n",
    "                    \"pm10_true\": float(pm10_tgt[ti, n, 0]),\n",
    "                })\n",
    "\n",
    "    return out_rows\n",
    "\n",
    "# Run all horizons\n",
    "all_pred_rows = []\n",
    "\n",
    "for h in HORIZONS:\n",
    "    model, scaler, times, X, c0, y, pm10_tgt, va_idx, te_idx = train_one_horizon(h)\n",
    "\n",
    "    tag = \"PI-GNN++\"\n",
    "    if (LAMBDA_INEQ > 0) or (LAMBDA_NONNEG > 0):\n",
    "        tag = \"PI-GNN++_constraints\"\n",
    "\n",
    "    all_pred_rows += predict_split(model, times, X, c0, y, pm10_tgt, va_idx, h, \"val\", tag)\n",
    "    all_pred_rows += predict_split(model, times, X, c0, y, pm10_tgt, te_idx, h, \"test\", tag)\n",
    "\n",
    "pred_df = pd.DataFrame(all_pred_rows)\n",
    "pred_df = pred_df.sort_values([\"split\",\"horizon_h\",\"model\",\"city_id\",\"datetime\"]).reset_index(drop=True)\n",
    "pred_df.to_csv(OUT_PRED_PATH, index=False)\n",
    "print(f\"\\nSaved: {OUT_PRED_PATH}  rows={len(pred_df)}  models={pred_df['model'].nunique()}\")\n",
    "\n",
    "if os.path.exists(BASELINE_PRED_PATH):\n",
    "    base = pd.read_csv(BASELINE_PRED_PATH, parse_dates=[\"datetime\"])\n",
    "    pred_df2 = pred_df.copy()\n",
    "    pred_df2[\"datetime\"] = pd.to_datetime(pred_df2[\"datetime\"])\n",
    "    combo = pd.concat([base, pred_df2], ignore_index=True)\n",
    "    combo.to_csv(OUT_ALL_PATH, index=False)\n",
    "    print(f\"Saved: {OUT_ALL_PATH}  rows={len(combo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcea58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
