{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41233a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2023-01-02 00:00:00 -> 2024-12-31 23:00:00 rows 508080\n",
      "Val  : 2025-01-01 00:00:00 -> 2025-06-30 23:00:00 rows 125976\n",
      "Test : 2025-07-01 00:00:00 -> 2025-11-22 23:00:00 rows 100920\n",
      "Epoch 1/30\n",
      "1983/1983 [==============================] - 67s 32ms/step - loss: 6.5259 - val_loss: 7.4031\n",
      "Epoch 2/30\n",
      "1983/1983 [==============================] - 63s 32ms/step - loss: 4.9065 - val_loss: 7.6299\n",
      "Epoch 3/30\n",
      "1983/1983 [==============================] - 63s 32ms/step - loss: 4.4108 - val_loss: 7.0231\n",
      "Epoch 4/30\n",
      "1983/1983 [==============================] - 63s 32ms/step - loss: 4.1861 - val_loss: 7.2096\n",
      "Epoch 5/30\n",
      "1983/1983 [==============================] - 65s 33ms/step - loss: 4.0242 - val_loss: 8.7122\n",
      "Epoch 6/30\n",
      "1983/1983 [==============================] - 66s 33ms/step - loss: 3.8955 - val_loss: 7.0651\n",
      "Epoch 1/30\n",
      "1983/1983 [==============================] - 70s 33ms/step - loss: 9.9880 - val_loss: 12.5313\n",
      "Epoch 2/30\n",
      "1983/1983 [==============================] - 64s 32ms/step - loss: 8.8739 - val_loss: 12.3367\n",
      "Epoch 3/30\n",
      "1983/1983 [==============================] - 64s 32ms/step - loss: 8.5619 - val_loss: 12.7464\n",
      "Epoch 4/30\n",
      "1983/1983 [==============================] - 65s 33ms/step - loss: 8.3177 - val_loss: 12.6102\n",
      "Epoch 5/30\n",
      "1983/1983 [==============================] - 65s 33ms/step - loss: 8.1168 - val_loss: 12.6434\n",
      "Epoch 1/30\n",
      "1983/1983 [==============================] - 100s 49ms/step - loss: 11.8066 - val_loss: 14.7933\n",
      "Epoch 2/30\n",
      "1983/1983 [==============================] - 77s 39ms/step - loss: 10.8608 - val_loss: 14.9372\n",
      "Epoch 3/30\n",
      "1983/1983 [==============================] - 77s 39ms/step - loss: 10.6065 - val_loss: 15.1294\n",
      "Epoch 4/30\n",
      "1983/1983 [==============================] - 77s 39ms/step - loss: 10.4047 - val_loss: 14.9094\n",
      "\n",
      "Saved: predictions_baselines.csv  rows=3403440  models=5\n",
      "\n",
      "=== TEST horizon h=1 ===\n",
      "HistGBR      | MAE   3.454 RMSE   5.770 R2  0.943 | viol 33.49% (mean  3.460)\n",
      "LSTM         | MAE   4.270 RMSE   6.064 R2  0.937 | viol 47.31% (mean  2.671)\n",
      "Persistence  | MAE   2.757 RMSE   5.044 R2  0.956 | viol 28.20% (mean  3.299)\n",
      "SeasonalNaive | MAE  12.214 RMSE  17.205 R2  0.493 | viol 51.50% (mean 10.441)\n",
      "VAR(6)       | MAE   2.079 RMSE   3.791 R2  0.975 | viol 29.85% (mean  2.284)\n",
      "\n",
      "=== TEST horizon h=6 ===\n",
      "HistGBR      | MAE   8.188 RMSE  12.361 R2  0.741 | viol 45.34% (mean  7.112)\n",
      "LSTM         | MAE   7.518 RMSE  11.298 R2  0.784 | viol 50.67% (mean  6.227)\n",
      "Persistence  | MAE  11.629 RMSE  18.624 R2  0.412 | viol 43.85% (mean 11.598)\n",
      "SeasonalNaive | MAE  12.262 RMSE  17.277 R2  0.494 | viol 51.44% (mean 10.452)\n",
      "VAR(6)       | MAE   8.801 RMSE  13.668 R2  0.683 | viol 46.77% (mean  8.101)\n",
      "\n",
      "=== TEST horizon h=24 ===\n",
      "HistGBR      | MAE  10.144 RMSE  15.484 R2  0.605 | viol 47.93% (mean  9.253)\n",
      "LSTM         | MAE   9.760 RMSE  14.953 R2  0.631 | viol 39.50% (mean  8.736)\n",
      "Persistence  | MAE   9.792 RMSE  15.564 R2  0.601 | viol 39.01% (mean 10.759)\n",
      "SeasonalNaive | MAE  12.370 RMSE  17.442 R2  0.498 | viol 51.25% (mean 10.483)\n",
      "VAR(6)       | MAE  12.892 RMSE  17.140 R2  0.516 | viol 65.85% (mean 11.154)\n"
     ]
    }
   ],
   "source": [
    "# BASELINE SUITE\n",
    "# Includes: Persistence, SeasonalNaive (FIXED for t+h), VAR, HistGBR, LSTM(Dropout+EarlyStop)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# LSTM \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Config\n",
    "DATA_PATH = \"dataset_2023_2025.csv\"\n",
    "HORIZONS = [1, 6, 24]\n",
    "LAGS = [1, 2, 6, 24]\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_END = \"2024-12-31 23:00:00\"\n",
    "VAL_END   = \"2025-06-30 23:00:00\"\n",
    "TEST_END  = \"2025-11-23 23:00:00\"\n",
    "\n",
    "# LSTM sequence settings\n",
    "SEQ_LEN = 24   # 24 hours history\n",
    "BATCH = 256\n",
    "EPOCHS = 30\n",
    "DROPOUT = 0.20\n",
    "LR = 1e-3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def make_lag_features(df, lags):\n",
    "    df = df.sort_values(['city_id','datetime']).copy()\n",
    "    for lag in lags:\n",
    "        df[f'pm2_5_lag{lag}'] = df.groupby('city_id')['pm2_5'].shift(lag)\n",
    "    return df\n",
    "\n",
    "def make_multi_horizon_targets(df, horizons):\n",
    "    df = df.sort_values(['city_id','datetime']).copy()\n",
    "    for h in horizons:\n",
    "        df[f'y_h{h}'] = df.groupby('city_id')['pm2_5'].shift(-h)\n",
    "        df[f'pm10_h{h}'] = df.groupby('city_id')['pm10'].shift(-h)  # constraint check only\n",
    "    return df\n",
    "\n",
    "def time_split(df):\n",
    "    train = df[df['datetime'] <= pd.Timestamp(TRAIN_END)].copy()\n",
    "    val = df[(df['datetime'] > pd.Timestamp(TRAIN_END)) & (df['datetime'] <= pd.Timestamp(VAL_END))].copy()\n",
    "    test = df[(df['datetime'] > pd.Timestamp(VAL_END)) & (df['datetime'] <= pd.Timestamp(TEST_END))].copy()\n",
    "    return train, val, test\n",
    "\n",
    "def constraint_violation_rate(pred_pm25, true_pm10):\n",
    "    v = pred_pm25 - true_pm10\n",
    "    viol = v > 0\n",
    "    rate = float(np.mean(viol))\n",
    "    mag = float(np.mean(v[viol])) if np.any(viol) else 0.0\n",
    "    return rate, mag\n",
    "\n",
    "# -----------------------------\n",
    "# Load\n",
    "# -----------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "keep_cols = [\n",
    "    'city_id','city_name','lat','lon','datetime',\n",
    "    'pm2_5','pm10',  # pm10 NOT used as input, only check\n",
    "    'carbon_monoxide','nitrogen_dioxide','sulphur_dioxide','ozone',\n",
    "    'doy_sin','doy_cos','hour_sin','hour_cos'\n",
    "]\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "# build lags + targets and drop NA\n",
    "df = make_lag_features(df, LAGS)\n",
    "df = make_multi_horizon_targets(df, HORIZONS)\n",
    "\n",
    "needed = [f'pm2_5_lag{l}' for l in LAGS] + [f'y_h{h}' for h in HORIZONS] + [f'pm10_h{h}' for h in HORIZONS]\n",
    "df = df.dropna(subset=needed).copy()\n",
    "\n",
    "train_df, val_df, test_df = time_split(df)\n",
    "\n",
    "print(\"Train:\", train_df['datetime'].min(), \"->\", train_df['datetime'].max(), \"rows\", len(train_df))\n",
    "print(\"Val  :\", val_df['datetime'].min(), \"->\", val_df['datetime'].max(), \"rows\", len(val_df))\n",
    "print(\"Test :\", test_df['datetime'].min(), \"->\", test_df['datetime'].max(), \"rows\", len(test_df))\n",
    "\n",
    "# -----------------------------\n",
    "# Feature definitions (PM10 excluded!)\n",
    "# -----------------------------\n",
    "num_features = [\n",
    "    'carbon_monoxide','nitrogen_dioxide','sulphur_dioxide','ozone',\n",
    "    'doy_sin','doy_cos','hour_sin','hour_cos',\n",
    "] + [f'pm2_5_lag{l}' for l in LAGS] + ['lat','lon']\n",
    "cat_features = ['city_id']\n",
    "\n",
    "def get_xy(dfi, h):\n",
    "    y = dfi[f'y_h{h}'].to_numpy(dtype=float)\n",
    "    pm10_t = dfi[f'pm10_h{h}'].to_numpy(dtype=float)\n",
    "    X = dfi[num_features + cat_features].copy()\n",
    "    return X, y, pm10_t\n",
    "\n",
    "# Baseline 0: Persistence (yhat = pm2_5 at time t)\n",
    "def predict_persistence(dfi):\n",
    "    return dfi['pm2_5'].to_numpy(dtype=float)\n",
    "\n",
    "# Baseline 1: Seasonal Naive\n",
    "def fit_seasonal_naive(train):\n",
    "    tmp = train.copy()\n",
    "    tmp['month'] = tmp['datetime'].dt.month\n",
    "    tmp['hour'] = tmp['datetime'].dt.hour\n",
    "    # city x month x hour climatology\n",
    "    clim = tmp.groupby(['city_id','month','hour'])['pm2_5'].mean()\n",
    "    # fallback city mean\n",
    "    city_mean = tmp.groupby('city_id')['pm2_5'].mean()\n",
    "    return clim, city_mean\n",
    "\n",
    "def predict_seasonal_naive(dfi, clim, city_mean, h):\n",
    "    tmp = dfi.copy()\n",
    "    dt_tgt = tmp['datetime'] + pd.to_timedelta(h, unit='h')\n",
    "    tmp['month_tgt'] = dt_tgt.dt.month\n",
    "    tmp['hour_tgt'] = dt_tgt.dt.hour\n",
    "\n",
    "    pred = np.empty(len(tmp), dtype=float)\n",
    "    # loop is OK; if you want vectorization later we can optimize\n",
    "    for i, r in enumerate(tmp.itertuples(index=False)):\n",
    "        key = (r.city_id, r.month_tgt, r.hour_tgt)\n",
    "        if key in clim.index:\n",
    "            pred[i] = float(clim.loc[key])\n",
    "        else:\n",
    "            pred[i] = float(city_mean.loc[r.city_id])\n",
    "    return pred\n",
    "\n",
    "# Baseline 2: VAR per city on [PM2.5, CO, NO2, SO2, O3]\n",
    "VAR_VARS = ['pm2_5','carbon_monoxide','nitrogen_dioxide','sulphur_dioxide','ozone']\n",
    "\n",
    "def fit_var_models(train, maxlags=6):\n",
    "    models = {}\n",
    "    for cid, g in train.sort_values('datetime').groupby('city_id'):\n",
    "        X = g[VAR_VARS].astype(float).to_numpy()\n",
    "        if len(X) < 200:\n",
    "            continue\n",
    "        m = VAR(X)\n",
    "        res = m.fit(maxlags=maxlags, ic=None, trend='c')\n",
    "        models[cid] = res\n",
    "    return models\n",
    "\n",
    "def predict_var_for_split(df_split, df_full, var_models, h):\n",
    "    # df_split: rows at time t (features); predict y(t+h)\n",
    "    out = np.full(len(df_split), np.nan, dtype=float)\n",
    "\n",
    "    for cid, g in df_split.sort_values(['city_id','datetime']).groupby('city_id'):\n",
    "        if cid not in var_models:\n",
    "            continue\n",
    "        res = var_models[cid]\n",
    "        k_ar = res.k_ar\n",
    "\n",
    "        full = df_full[df_full['city_id']==cid].sort_values('datetime')[['datetime']+VAR_VARS].copy()\n",
    "        full_dt = full['datetime'].to_numpy()\n",
    "        full_X  = full[VAR_VARS].to_numpy(dtype=float)\n",
    "\n",
    "        pos = pd.Series(np.arange(len(full_dt)), index=full_dt)\n",
    "\n",
    "        for idx, row in g.iterrows():\n",
    "            t = row['datetime']\n",
    "            if t not in pos.index:\n",
    "                continue\n",
    "            p = int(pos.loc[t])\n",
    "            if p - k_ar + 1 < 0:\n",
    "                continue\n",
    "            hist = full_X[p-k_ar+1:p+1]\n",
    "            fc = res.forecast(hist, steps=h)\n",
    "            out[df_split.index.get_loc(idx)] = fc[-1, 0]  # PM2.5\n",
    "    return out\n",
    "\n",
    "# Baseline 3: Fast tree (HistGBR)\n",
    "def build_tree_pipeline():\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), num_features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "        ]\n",
    "    )\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_depth=8,\n",
    "        learning_rate=0.08,\n",
    "        max_iter=400,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    return Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "# Baseline 4: LSTM (Dropout + EarlyStopping)\n",
    "# Sequence per city: last SEQ_LEN hours of [pm2.5 + chems + time feats]\n",
    "# Static: lat/lon + city_id one-hot\n",
    "SEQ_FEATURES = [\n",
    "    'pm2_5',  # NOTE: uses observed history only up to time t\n",
    "    'carbon_monoxide','nitrogen_dioxide','sulphur_dioxide','ozone',\n",
    "    'doy_sin','doy_cos','hour_sin','hour_cos'\n",
    "]\n",
    "STATIC_NUM = ['lat','lon']\n",
    "STATIC_CAT = ['city_id']\n",
    "\n",
    "def build_seq_index(dfi):\n",
    "    # dfi must be sorted by city/time\n",
    "    dfi = dfi.sort_values(['city_id','datetime']).copy()\n",
    "    dfi['pos_in_city'] = dfi.groupby('city_id').cumcount()\n",
    "    return dfi\n",
    "\n",
    "def make_lstm_arrays(df_full, df_split, h, seq_len=24):\n",
    "    \"\"\"\n",
    "    Build sequences for rows in df_split (at time t), using df_full history up to t.\n",
    "    Returns: X_seq, X_static, y, meta (city_id, datetime, pm10_true)\n",
    "    \"\"\"\n",
    "    df_full = build_seq_index(df_full)\n",
    "    df_split = df_split.sort_values(['city_id','datetime']).copy()\n",
    "    city_ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    city_ohe.fit(train_df[['city_id']])\n",
    "\n",
    "    X_seq_list, X_stat_list, y_list, pm10_list = [], [], [], []\n",
    "    meta_city, meta_dt = [], []\n",
    "\n",
    "    for cid, g in df_split.groupby('city_id'):\n",
    "        full_c = df_full[df_full['city_id']==cid].sort_values('datetime')\n",
    "        full_dt = full_c['datetime'].to_numpy()\n",
    "        full_X  = full_c[SEQ_FEATURES].to_numpy(dtype=float)\n",
    "\n",
    "        # map datetime->index\n",
    "        pos = pd.Series(np.arange(len(full_dt)), index=full_dt)\n",
    "\n",
    "        # static for this city (same for all)\n",
    "        lat = float(g['lat'].iloc[0]); lon = float(g['lon'].iloc[0])\n",
    "        cid_ohe = city_ohe.transform(pd.DataFrame({'city_id':[cid]}))[0]\n",
    "        stat_vec = np.concatenate([[lat, lon], cid_ohe], axis=0)\n",
    "\n",
    "        for r in g.itertuples(index=False):\n",
    "            t = r.datetime\n",
    "            if t not in pos.index:\n",
    "                continue\n",
    "            p = int(pos.loc[t])\n",
    "            if p - seq_len + 1 < 0:\n",
    "                continue\n",
    "\n",
    "            # sequence ends at t (inclusive), length seq_len\n",
    "            seq = full_X[p-seq_len+1:p+1]  # (seq_len, nfeat)\n",
    "\n",
    "            # target is stored already in df_split row as y_h{h}\n",
    "            y_t = getattr(r, f'y_h{h}')\n",
    "            pm10_t = getattr(r, f'pm10_h{h}')\n",
    "\n",
    "            X_seq_list.append(seq)\n",
    "            X_stat_list.append(stat_vec)\n",
    "            y_list.append(float(y_t))\n",
    "            pm10_list.append(float(pm10_t))\n",
    "            meta_city.append(cid)\n",
    "            meta_dt.append(t)\n",
    "\n",
    "    X_seq = np.stack(X_seq_list).astype(np.float32)\n",
    "    X_stat = np.stack(X_stat_list).astype(np.float32)\n",
    "    y = np.array(y_list, dtype=np.float32)\n",
    "    pm10_true = np.array(pm10_list, dtype=np.float32)\n",
    "\n",
    "    meta = pd.DataFrame({'city_id': meta_city, 'datetime': meta_dt})\n",
    "    return X_seq, X_stat, y, pm10_true, meta\n",
    "\n",
    "def build_lstm_model(seq_len, n_seq_feat, n_static_feat):\n",
    "    inp_seq = layers.Input(shape=(seq_len, n_seq_feat), name=\"seq\")\n",
    "    x = layers.Masking()(inp_seq)\n",
    "    x = layers.LSTM(64, return_sequences=False, dropout=DROPOUT, recurrent_dropout=0.0)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "\n",
    "    inp_stat = layers.Input(shape=(n_static_feat,), name=\"static\")\n",
    "    s = layers.Dense(32, activation=\"relu\")(inp_stat)\n",
    "    s = layers.Dropout(DROPOUT)(s)\n",
    "\n",
    "    z = layers.Concatenate()([x, s])\n",
    "    z = layers.Dense(64, activation=\"relu\")(z)\n",
    "    z = layers.Dropout(DROPOUT)(z)\n",
    "    out = layers.Dense(1, name=\"y\")(z)\n",
    "\n",
    "    model = Model([inp_seq, inp_stat], out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LR),\n",
    "        loss=\"mae\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Run baselines and SAVE predictions\n",
    "pred_rows = []\n",
    "\n",
    "def add_preds(model_name, split_name, h, meta_df, y_true, y_pred, pm10_true):\n",
    "    # long format rows for saving\n",
    "    out = meta_df.copy()\n",
    "    out['split'] = split_name\n",
    "    out['model'] = model_name\n",
    "    out['horizon_h'] = h\n",
    "    out['y_true'] = y_true\n",
    "    out['y_pred'] = y_pred\n",
    "    out['pm10_true'] = pm10_true\n",
    "    pred_rows.append(out)\n",
    "\n",
    "# Persistence + SeasonalNaive\n",
    "clim, city_mean = fit_seasonal_naive(train_df)\n",
    "\n",
    "for h in HORIZONS:\n",
    "    # VAL\n",
    "    meta_val = val_df[['city_id','datetime']].copy()\n",
    "    yv = val_df[f'y_h{h}'].to_numpy(float)\n",
    "    pm10v = val_df[f'pm10_h{h}'].to_numpy(float)\n",
    "\n",
    "    add_preds(\"Persistence\", \"val\", h, meta_val, yv, predict_persistence(val_df), pm10v)\n",
    "    add_preds(\"SeasonalNaive\", \"val\", h, meta_val, yv, predict_seasonal_naive(val_df, clim, city_mean, h), pm10v)\n",
    "\n",
    "    # TEST\n",
    "    meta_te = test_df[['city_id','datetime']].copy()\n",
    "    yt = test_df[f'y_h{h}'].to_numpy(float)\n",
    "    pm10t = test_df[f'pm10_h{h}'].to_numpy(float)\n",
    "\n",
    "    add_preds(\"Persistence\", \"test\", h, meta_te, yt, predict_persistence(test_df), pm10t)\n",
    "    add_preds(\"SeasonalNaive\", \"test\", h, meta_te, yt, predict_seasonal_naive(test_df, clim, city_mean, h), pm10t)\n",
    "\n",
    "# VAR \n",
    "var_models = fit_var_models(train_df, maxlags=6)\n",
    "for h in HORIZONS:\n",
    "    # VAL\n",
    "    yhat_val = predict_var_for_split(val_df, df, var_models, h)\n",
    "    miss = np.isnan(yhat_val)\n",
    "    if miss.any():\n",
    "        yhat_val[miss] = predict_persistence(val_df.iloc[np.where(miss)[0]])\n",
    "    add_preds(\"VAR(6)\", \"val\", h,\n",
    "              val_df[['city_id','datetime']].copy(),\n",
    "              val_df[f'y_h{h}'].to_numpy(float),\n",
    "              yhat_val,\n",
    "              val_df[f'pm10_h{h}'].to_numpy(float))\n",
    "\n",
    "    # TEST\n",
    "    yhat_test = predict_var_for_split(test_df, df, var_models, h)\n",
    "    miss = np.isnan(yhat_test)\n",
    "    if miss.any():\n",
    "        yhat_test[miss] = predict_persistence(test_df.iloc[np.where(miss)[0]])\n",
    "    add_preds(\"VAR(6)\", \"test\", h,\n",
    "              test_df[['city_id','datetime']].copy(),\n",
    "              test_df[f'y_h{h}'].to_numpy(float),\n",
    "              yhat_test,\n",
    "              test_df[f'pm10_h{h}'].to_numpy(float))\n",
    "\n",
    "# HistGBR \n",
    "for h in HORIZONS:\n",
    "    X_tr, y_tr, _ = get_xy(train_df, h)\n",
    "    X_va, y_va, pm10_va = get_xy(val_df, h)\n",
    "    X_te, y_te, pm10_te = get_xy(test_df, h)\n",
    "\n",
    "    pipe = build_tree_pipeline()\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    yhat_va = pipe.predict(X_va)\n",
    "    yhat_te = pipe.predict(X_te)\n",
    "\n",
    "    add_preds(\"HistGBR\", \"val\", h, val_df[['city_id','datetime']].copy(), y_va, yhat_va, pm10_va)\n",
    "    add_preds(\"HistGBR\", \"test\", h, test_df[['city_id','datetime']].copy(), y_te, yhat_te, pm10_te)\n",
    "\n",
    "# LSTM \n",
    "# Scale SEQ features using TRAIN ONLY (leakage-safe)\n",
    "seq_scaler = StandardScaler()\n",
    "seq_scaler.fit(train_df[SEQ_FEATURES].to_numpy(float))\n",
    "\n",
    "def apply_seq_scaler(df_in):\n",
    "    df2 = df_in.copy()\n",
    "    df2[SEQ_FEATURES] = seq_scaler.transform(df2[SEQ_FEATURES].to_numpy(float))\n",
    "    return df2\n",
    "\n",
    "df_scaled = apply_seq_scaler(df)\n",
    "train_scaled = apply_seq_scaler(train_df)\n",
    "val_scaled = apply_seq_scaler(val_df)\n",
    "test_scaled = apply_seq_scaler(test_df)\n",
    "\n",
    "# Build static dimension size\n",
    "tmp_ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False).fit(train_df[['city_id']])\n",
    "n_static = len(STATIC_NUM) + tmp_ohe.transform(train_df[['city_id']].iloc[:1]).shape[1]\n",
    "\n",
    "for h in HORIZONS:\n",
    "    # Build arrays\n",
    "    Xs_tr, Xst_tr, y_tr, pm10_tr, meta_tr = make_lstm_arrays(df_scaled, train_scaled, h, seq_len=SEQ_LEN)\n",
    "    Xs_va, Xst_va, y_va, pm10_va, meta_va = make_lstm_arrays(df_scaled, val_scaled, h, seq_len=SEQ_LEN)\n",
    "    Xs_te, Xst_te, y_te, pm10_te, meta_te = make_lstm_arrays(df_scaled, test_scaled, h, seq_len=SEQ_LEN)\n",
    "\n",
    "    model = build_lstm_model(SEQ_LEN, Xs_tr.shape[-1], n_static)\n",
    "\n",
    "    cb = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        {\"seq\": Xs_tr, \"static\": Xst_tr}, y_tr,\n",
    "        validation_data=({\"seq\": Xs_va, \"static\": Xst_va}, y_va),\n",
    "        epochs=EPOCHS, batch_size=BATCH, callbacks=cb, verbose=1\n",
    "    )\n",
    "\n",
    "    yhat_va = model.predict({\"seq\": Xs_va, \"static\": Xst_va}, batch_size=BATCH).reshape(-1)\n",
    "    yhat_te = model.predict({\"seq\": Xs_te, \"static\": Xst_te}, batch_size=BATCH).reshape(-1)\n",
    "\n",
    "    add_preds(\"LSTM\", \"val\", h, meta_va, y_va, yhat_va, pm10_va)\n",
    "    add_preds(\"LSTM\", \"test\", h, meta_te, y_te, yhat_te, pm10_te)\n",
    "\n",
    "# Save predictions\n",
    "pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "pred_df = pred_df.sort_values(['split','horizon_h','model','city_id','datetime']).reset_index(drop=True)\n",
    "\n",
    "OUT_PATH = \"predictions_baselines.csv\"\n",
    "pred_df.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved: {OUT_PATH}  rows={len(pred_df)}  models={pred_df['model'].nunique()}\")\n",
    "\n",
    "# Quick sanity summary on TEST \n",
    "def summarize(pred_df, split=\"test\"):\n",
    "    sub = pred_df[pred_df['split']==split]\n",
    "    for h in sorted(sub['horizon_h'].unique()):\n",
    "        print(f\"\\n=== {split.upper()} horizon h={h} ===\")\n",
    "        for m in sub['model'].unique():\n",
    "            s = sub[(sub['horizon_h']==h) & (sub['model']==m)]\n",
    "            y = s['y_true'].to_numpy(float)\n",
    "            yhat = s['y_pred'].to_numpy(float)\n",
    "            pm10 = s['pm10_true'].to_numpy(float)\n",
    "            mae = mean_absolute_error(y, yhat)\n",
    "            r = rmse(y, yhat)\n",
    "            r2 = r2_score(y, yhat)\n",
    "            vr, vm = constraint_violation_rate(yhat, pm10)\n",
    "            print(f\"{m:12s} | MAE {mae:7.3f} RMSE {r:7.3f} R2 {r2:6.3f} | viol {100*vr:5.2f}% (mean {vm:6.3f})\")\n",
    "\n",
    "summarize(pred_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448943ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
